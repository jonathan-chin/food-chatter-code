# Food Chatter NLP Analytics

## Directory Structure

- **/data**
  - **/raw**
	- **talkwalker** - raw csv dumps from Talkwalker. these are tweets pulled between March 2020 and March 2021 according to the search keywords outlined in our research
	- **exclusion_keywords.csv** - manually generated list of blacklist keywords that the research team pulled together after an initial look at the data. Talkwalker gave us way too many tweets and these keywords helped to reduce the dataset.
  - **/generated** - all intermediate and final data files generated off of the raw ones are stored here. this entire directory can be regenerated by running the appropriate jupyter notebook and python script
- **gsdmm** - a static clone of the gsdmm code on github initially provided by [rwalk](https://github.com/rwalk/gsdmm). additional code was retrieved from 
[dtemkim](https://github.com/dtemkin/gsdmm) and modified by Jon Chin to allow for saving and loading of models


## 01 Filtering Raw Data

to start, run all of the `filter_keywords_*.ipynb` notebooks. each file pulls a subsection of the raw dataset and does the following:

1. discards some of the original columns
2. renames several columns for convenience
3. discards records that have any of the following talkwalker category tags, such as real_estate and job_offers
4. applies a dataset-universal filter of exclusion keywords. any records that have any of the exclusion keywords in their content column (ie the tweet body) are discarded.
5. upon manual review, **each** individual notebook has an additional list of exclusion keywords that are applied to that subsection of the dataset.
6. performs other data cleaning tasks, such as removing any @mentions and any records with less than 15 characters
7. saves the resulting dataset to `data/generated/filtered_*.csv`

## 02 Preprocess Manually Scored Twitter Usernames

next, run the `manual_scores_preprocess.ipnyb` notebook. this takes in a raw data file named `data/raw/manual_scored_twitter_usernames.csv` that was generated by having 3 research assistants go through a sample of tweets and score them as bot or not. this sums up the votes and produces a single file named `data/generated/coalesced_manual_scored_twitter_usernames.csv`.

## 04 Generate List of Suspected Bot Usernames

next, run the `calculate_cutoff.ipnyb` notebook. this uses the csv generated in the previous step to arrive at a botometer score threshold based on our manual scoring and then apply the usernames in `data/raw/scored_twitter_usernames.csv`. this csv was generated by running all of the unique usernames in our dataset against the [Botometer](https://botometer.osome.iu.edu/) api. the notebook throws out any usernames with a score below the cutoff score and save the usernames most likely to be human to `data/generated/botometer_usernames_by_cutoff.csv`

## 05 Combining Filtered Datasets

next, run the `throw_out_bots_and_combine.ipnyb` notebook. this combines all of the `data/generated/filtered_*.csv` files. it also opens the csv file generated in the previous step and discard any tweets from usernames that are **not** present in both datasets. as a convenience, this notebook also rekeys the **matched_profile** column to shorter, easier to read values in the `cluster` column. the results are saved to `data/generated/debotted_combined.csv`

## 06 Generating Supplementary Datasets

depending on your research objectives, you can optionally run `distinct_and_pull_most_duplicated.ipnyb`. this produces 3 files:

- `data/generated/top_20_duplicated_tweets.csv` that contains the top 20 retweeted tweets
- `data/generated/top_20_duplicated_tweets_with_dates.csv` that contains the same but with a dates column (we can probably keep this in lieu of the previous file)
- `data/generated/debotted_combined_distinct.csv` which is the `debotted_combined` dataset with duplicate tweets removed

you can also run `remove_major_rts.ipnyb` to remove all tweets that start with `RT`. this produces `data/generated/debotted_combined_no_rts.csv`

## 07 Tokenize

there are 3 provided tokenize notebooks: `tokenize_distinct.ipnyb`, `tokenize_no_rts.ipnyb`, and `tokenize.ipnyb`. they all perform the same general functions:

- remove `RT` from the beginning of all tweets
- convert all tweets to lowercase
- replace all contractions with their expanded forms
- remove all urls
- remove all `@` mentions
- remove all `$` mentions, which usually are Cash App usernames
- remove all punctuation
- remove extra whitespace
- convert all words to their stems
- remove stopwords
- divvy up tweets based on their cluster name (except for `tokenize_distinct.ipnyb`)

`tokenize.ipnyb` does this with the whole dataset, ie `debotted_combined.csv`. there are 2 additional variations: `_distinct` and `_no_rts`. the following files are produced:

- `data/generated/data_with_tokenized_${cluster_name}.csv`
- `data/generated/data_with_tokenized_${cluster_name}.txt`
- `data/generated/data_with_tokenized_distinct.csv`
- `data/generated/data_with_tokenized_distinct.txt`
- `data/generated/data_with_tokenized_no_rts_${cluster_name}.csv`
- `data/generated/data_with_tokenized_no_rts_${cluster_name}.txt`

the `txt` files are needed to produce the topic modelling pickle (more on that in the next step); the `csv` files are needed to reverse score each tweet and assign them a topic.

### note

our research team decided to focus just on the distinct dataset. the other 2 methodologies (no_rts and plain tokenize) are deprecated and probably need updating. for the rest of this guide, we will only focus on the distinct dataset. the distinct dataset also produces a single result file instead of several separated by cluster name.

## 08 Topic Modeling

next, run `topic_modeling_distinct.py`. this loads in `data/generated/tokenized_distinct.txt` and use the `GSDMM` algorithm with k = 50 to produce `data/generated/mgp_distinct.pickle`. the pickle file can be loaded later to categorize tweets based on the training data. this also relies on the provided `gsdmm` directory, which is a lightly modified version of [rwalk's code](https://github.com/rwalk/gsdmm).

provided are also `topic_modeling_gsdmm.py` and `topic_modeling_no_rts_gsdmm.py` but those are deprecated as well. they are provided only for reference.

## 09 Categorizing Dataset

next, run `gsdmm_score.py`. this loads the `data/generated/tokenized_distinct.csv` file as well as `data/generated/mgp_distinct.pickle`. it uses the model from the previous step to score each tweet and assign it to a topic. this is necessary because the GSDMM algorithm only provides information about the produced clusters, ie the tokens that characterize a cluster. these tokens are separated from their original tweets. by scoring the dataset again with the generated model, we can attach topics to tweets in the original dataset. this also relies on the provided `gsdmm` directory, which is a lightly modified version of [rwalk's code](https://github.com/rwalk/gsdmm).

this produces a new file `data/generated/content_with_most_likely_cluster.csv`. note: there are no matching scoring files for the general and no_rts datasets.

## 10 Coalesce into Major Topics

next, run `analyze_gsdmm_scored.ipnyb`. this combines topics into larger, main topics. the mapping of topic to main topic was determined by our research team and would likely need to be edited to use with other datasets. a new file is generated: `data/generated/gsdmm_scored_by_theme.csv`. this is the last generated data file and is what we use for visualizations and interpretation.

## 11 Generate Word Cloud

to generate a wordcloud for easier data visualization, run `wordcloud_preprocessing.js`. this uses the `data/generated/debotted_combined.csv` and `data/generated/debotted_combined_distinct.csv` to produce two files: `data/generated/wordcloud_raw.txt` and `data/generated/wordcloud_raw_distinct.txt`. these generated txt files contain just the content column from the dataset.

then, run the `generated_wordcloud` script in `package.json`. there are supplementary scripts `generate_wordcloud:distinct` and `generate_wordcloud:distinct:no_min`.

**note**: our team ultimately only ended up using the `distinct` variations. the other variations are provided for reference and are not guaranteed to work.

## 12 Supplementary Files

there are a handful of additional files that 

- `get_all_usernames_as_twitter_urls.ipnyb` takes all the twitter usernames in our dataset and transforms them into a url to view the user's profile. this is saved in `data/generated/urls.csv`
- `sample_urls_for_mturk.ipnyb` extracts a random sample of 0.2% for manual bot categorization through AWS MTurk. it uses the generated file from `get_all_usernames_as_twitter_urls.ipnyb` and saves the sampled urls to `data/generated/mturk_urls.csv`
- `analyze_mturk_results.ipnyb` takes the file `data/raw/mturk_results.csv` (downloaded from AWS MTurk) to help establish a Botometer cutoff score using a confusion matrix
- `analyze_scored_twitter_usernames.ipnyb` produces descriptive statistics on `data/raw/scored_twitter_usernames.csv`, which is purely informational.
- `extract_keywords_by_cluster.js` processes each of the `data/generated/report_${cluster}.txt` files to extract their keywords and save them in `data/generated/cluster_keywords_${cluster}.csv`. there are 2 variations of this file: `_distinct` and `no_rts`.
- `extract_tweets_by_cluster_keywords.ipnyb` uses the generated file in `extract_keywords_by_cluster.js` to roughly divide the original tweet dataset by how well they match the keywords from the topic modeling clusters. this produces several `data/generated/tweets_by_cluster_${cluster_id}.csv` files, which were used by the research team to spot-check the validity of generated clusters. there are 2 variations of this file: `_distinct` and `no_rts`.
- `generate_frequency_graphs.ipnyb` is an early attempt to visualize the distribution of tweets over time; this is replaced with our final data visualization solution using Google's Looker Studio
- `retrieve_botometer_scores.ipnyb` calls the Botometer API with all the distinct usernames in our dataset. it saves the results in `data/generated/twitter/usernames_botometer.csv`
